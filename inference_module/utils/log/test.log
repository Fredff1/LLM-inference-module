[2025-04-16 20:39:37] [INFO] 日志系统已初始化
[2025-04-16 20:39:37] [INFO] 日志文件路径: d:\Code\Python\LLM-inference\inference_module\utils\log\test.log
[2025-04-16 20:39:37] [INFO] Initializing ModelInference with the following configuration:
[2025-04-16 20:39:37] [INFO] model_name: qwen2.5-0.5B
[2025-04-16 20:39:37] [INFO] model_path: D:\Files\Models\
[2025-04-16 20:39:37] [INFO] chat_type: classical
[2025-04-16 20:39:37] [INFO] model_generate_args:
[2025-04-16 20:39:37] [INFO] model_generate_args.max_new_tokens: 1024
[2025-04-16 20:39:37] [INFO] model_generate_args.temperature: 1.0
[2025-04-16 20:39:37] [INFO] model_generate_args.top_p: 1.0
[2025-04-16 20:39:37] [INFO] model_generate_args.top_k: 50
[2025-04-16 20:39:37] [INFO] model_generate_args.do_sample: True
[2025-04-16 20:39:37] [INFO] model_generate_args.num_beams: 1
[2025-04-16 20:39:37] [INFO] model_generate_args.length_penalty: 1.0
[2025-04-16 20:39:37] [INFO] model_generate_args.use_cache: True
[2025-04-16 20:39:37] [INFO] model_generate_args.repetition_penalty: 1.0
[2025-04-16 20:39:37] [INFO] tokenizer_generate_args:
[2025-04-16 20:39:37] [INFO] tokenizer_generate_args.padding: True
[2025-04-16 20:39:37] [INFO] tokenizer_generate_args.add_special_tokens: True
[2025-04-16 20:39:37] [INFO] tokenizer_generate_args.return_tensors: pt
[2025-04-16 20:39:37] [INFO] model_init_args:
[2025-04-16 20:39:37] [INFO] model_init_args.trust_remote_code: True
[2025-04-16 20:39:37] [INFO] model_init_args.torch_dtype: auto
[2025-04-16 20:39:37] [INFO] model_init_args.cache_dir: None
[2025-04-16 20:39:37] [INFO] apply_chat_template: False
[2025-04-16 20:39:37] [INFO] device_config:
[2025-04-16 20:39:37] [INFO] device_config.device: cuda:0
[2025-04-16 20:39:37] [INFO] device_id: 0
[2025-04-16 20:42:14] [INFO] 日志系统已初始化
[2025-04-16 20:42:14] [INFO] 日志文件路径: d:\Code\Python\LLM-inference\inference_module\utils\log\test.log
[2025-04-16 20:42:14] [INFO] Initializing ModelInference with the following configuration:
[2025-04-16 20:42:14] [INFO] model_name: qwen2.5-0.5B
[2025-04-16 20:42:14] [INFO] model_path: D:\Files\Models\
[2025-04-16 20:42:14] [INFO] chat_type: classical
[2025-04-16 20:42:14] [INFO] model_generate_args:
[2025-04-16 20:42:14] [INFO] model_generate_args.max_new_tokens: 1024
[2025-04-16 20:42:14] [INFO] model_generate_args.temperature: 1.0
[2025-04-16 20:42:14] [INFO] model_generate_args.top_p: 1.0
[2025-04-16 20:42:14] [INFO] model_generate_args.top_k: 50
[2025-04-16 20:42:14] [INFO] model_generate_args.do_sample: True
[2025-04-16 20:42:14] [INFO] model_generate_args.num_beams: 1
[2025-04-16 20:42:14] [INFO] model_generate_args.length_penalty: 1.0
[2025-04-16 20:42:14] [INFO] model_generate_args.use_cache: True
[2025-04-16 20:42:14] [INFO] model_generate_args.repetition_penalty: 1.0
[2025-04-16 20:42:14] [INFO] tokenizer_generate_args:
[2025-04-16 20:42:14] [INFO] tokenizer_generate_args.padding: True
[2025-04-16 20:42:14] [INFO] tokenizer_generate_args.add_special_tokens: True
[2025-04-16 20:42:14] [INFO] tokenizer_generate_args.return_tensors: pt
[2025-04-16 20:42:14] [INFO] model_init_args:
[2025-04-16 20:42:14] [INFO] model_init_args.trust_remote_code: True
[2025-04-16 20:42:14] [INFO] model_init_args.torch_dtype: auto
[2025-04-16 20:42:14] [INFO] model_init_args.cache_dir: None
[2025-04-16 20:42:14] [INFO] apply_chat_template: False
[2025-04-16 20:42:14] [INFO] device_config:
[2025-04-16 20:42:14] [INFO] device_config.device: cuda:0
[2025-04-16 20:42:14] [INFO] device_id: 0
[2025-04-16 20:42:43] [INFO] 日志系统已初始化
[2025-04-16 20:42:43] [INFO] 日志文件路径: d:\Code\Python\LLM-inference\inference_module\utils\log\test.log
[2025-04-16 20:42:43] [INFO] Initializing ModelInference with the following configuration:
[2025-04-16 20:42:43] [INFO] model_name: qwen2.5-0.5B
[2025-04-16 20:42:43] [INFO] model_path: D:\Files\Models\
[2025-04-16 20:42:43] [INFO] chat_type: classical
[2025-04-16 20:42:43] [INFO] model_generate_args:
[2025-04-16 20:42:43] [INFO] model_generate_args.max_new_tokens: 1024
[2025-04-16 20:42:43] [INFO] model_generate_args.temperature: 1.0
[2025-04-16 20:42:43] [INFO] model_generate_args.top_p: 1.0
[2025-04-16 20:42:43] [INFO] model_generate_args.top_k: 50
[2025-04-16 20:42:43] [INFO] model_generate_args.do_sample: True
[2025-04-16 20:42:43] [INFO] model_generate_args.num_beams: 1
[2025-04-16 20:42:43] [INFO] model_generate_args.length_penalty: 1.0
[2025-04-16 20:42:43] [INFO] model_generate_args.use_cache: True
[2025-04-16 20:42:43] [INFO] model_generate_args.repetition_penalty: 1.0
[2025-04-16 20:42:43] [INFO] tokenizer_generate_args:
[2025-04-16 20:42:43] [INFO] tokenizer_generate_args.padding: True
[2025-04-16 20:42:43] [INFO] tokenizer_generate_args.add_special_tokens: True
[2025-04-16 20:42:43] [INFO] tokenizer_generate_args.return_tensors: pt
[2025-04-16 20:42:43] [INFO] model_init_args:
[2025-04-16 20:42:43] [INFO] model_init_args.trust_remote_code: True
[2025-04-16 20:42:43] [INFO] model_init_args.torch_dtype: auto
[2025-04-16 20:42:43] [INFO] model_init_args.cache_dir: None
[2025-04-16 20:42:43] [INFO] apply_chat_template: False
[2025-04-16 20:42:43] [INFO] device_config:
[2025-04-16 20:42:43] [INFO] device_config.device: cuda:0
[2025-04-16 20:42:43] [INFO] device_id: 0
[2025-04-16 21:13:21] [INFO] 日志系统已初始化
[2025-04-16 21:13:21] [INFO] 日志文件路径: d:\Code\Python\LLM-inference\inference_module\utils\log\test.log
[2025-04-16 21:13:21] [INFO] Initializing ModelInference with the following configuration:
[2025-04-16 21:13:21] [INFO] model_name: qwen2.5-0.5B
[2025-04-16 21:13:21] [INFO] model_path: D:\Files\Models\
[2025-04-16 21:13:21] [INFO] chat_type: classical
[2025-04-16 21:13:21] [INFO] model_generate_args:
[2025-04-16 21:13:21] [INFO] model_generate_args.max_new_tokens: 1024
[2025-04-16 21:13:21] [INFO] model_generate_args.temperature: 1.0
[2025-04-16 21:13:21] [INFO] model_generate_args.top_p: 1.0
[2025-04-16 21:13:21] [INFO] model_generate_args.top_k: 50
[2025-04-16 21:13:21] [INFO] model_generate_args.do_sample: True
[2025-04-16 21:13:21] [INFO] model_generate_args.num_beams: 1
[2025-04-16 21:13:21] [INFO] model_generate_args.length_penalty: 1.0
[2025-04-16 21:13:21] [INFO] model_generate_args.use_cache: True
[2025-04-16 21:13:21] [INFO] model_generate_args.repetition_penalty: 1.0
[2025-04-16 21:13:21] [INFO] tokenizer_generate_args:
[2025-04-16 21:13:21] [INFO] tokenizer_generate_args.padding: True
[2025-04-16 21:13:21] [INFO] tokenizer_generate_args.add_special_tokens: True
[2025-04-16 21:13:21] [INFO] tokenizer_generate_args.return_tensors: pt
[2025-04-16 21:13:21] [INFO] model_init_args:
[2025-04-16 21:13:21] [INFO] model_init_args.trust_remote_code: True
[2025-04-16 21:13:21] [INFO] model_init_args.torch_dtype: auto
[2025-04-16 21:13:21] [INFO] model_init_args.cache_dir: None
[2025-04-16 21:13:21] [INFO] apply_chat_template: False
[2025-04-16 21:13:21] [INFO] device_config:
[2025-04-16 21:13:21] [INFO] device_config.device: cuda:0
[2025-04-16 21:13:21] [INFO] device_id: 0
[2025-04-16 21:17:28] [INFO] 日志系统已初始化
[2025-04-16 21:17:28] [INFO] 日志文件路径: d:\Code\Python\LLM-inference\inference_module\utils\log\test.log
[2025-04-16 21:17:28] [INFO] Initializing ModelInference with the following configuration:
[2025-04-16 21:17:28] [INFO] model_name: qwen2.5-0.5B
[2025-04-16 21:17:28] [INFO] model_path: D:\Files\Models\
[2025-04-16 21:17:28] [INFO] chat_type: classical
[2025-04-16 21:17:28] [INFO] model_generate_args:
[2025-04-16 21:17:28] [INFO] model_generate_args.max_new_tokens: 1024
[2025-04-16 21:17:28] [INFO] model_generate_args.temperature: 1.0
[2025-04-16 21:17:28] [INFO] model_generate_args.top_p: 1.0
[2025-04-16 21:17:28] [INFO] model_generate_args.top_k: 50
[2025-04-16 21:17:28] [INFO] model_generate_args.do_sample: True
[2025-04-16 21:17:28] [INFO] model_generate_args.num_beams: 1
[2025-04-16 21:17:28] [INFO] model_generate_args.length_penalty: 1.0
[2025-04-16 21:17:28] [INFO] model_generate_args.use_cache: True
[2025-04-16 21:17:28] [INFO] model_generate_args.repetition_penalty: 1.0
[2025-04-16 21:17:28] [INFO] tokenizer_generate_args:
[2025-04-16 21:17:28] [INFO] tokenizer_generate_args.padding: True
[2025-04-16 21:17:28] [INFO] tokenizer_generate_args.add_special_tokens: True
[2025-04-16 21:17:28] [INFO] tokenizer_generate_args.return_tensors: pt
[2025-04-16 21:17:28] [INFO] tokenizer_init_args:
[2025-04-16 21:17:28] [INFO] tokenizer_init_args.padding: True
[2025-04-16 21:17:28] [INFO] tokenizer_init_args.add_special_tokens: True
[2025-04-16 21:17:28] [INFO] tokenizer_init_args.return_tensors: pt
[2025-04-16 21:17:28] [INFO] model_init_args:
[2025-04-16 21:17:28] [INFO] model_init_args.trust_remote_code: True
[2025-04-16 21:17:28] [INFO] model_init_args.torch_dtype: auto
[2025-04-16 21:17:28] [INFO] model_init_args.cache_dir: None
[2025-04-16 21:17:28] [INFO] apply_chat_template: False
[2025-04-16 21:17:28] [INFO] device_config:
[2025-04-16 21:17:28] [INFO] device_config.device: cuda:0
[2025-04-16 21:17:28] [INFO] device_id: 0
[2025-04-16 21:18:21] [INFO] 日志系统已初始化
[2025-04-16 21:18:21] [INFO] 日志文件路径: d:\Code\Python\LLM-inference\inference_module\utils\log\test.log
[2025-04-16 21:18:21] [INFO] Initializing ModelInference with the following configuration:
[2025-04-16 21:18:21] [INFO] model_name: qwen2.5-0.5B
[2025-04-16 21:18:21] [INFO] model_path: D:\Files\Models\
[2025-04-16 21:18:21] [INFO] chat_type: classical
[2025-04-16 21:18:21] [INFO] model_generate_args:
[2025-04-16 21:18:21] [INFO] model_generate_args.max_new_tokens: 1024
[2025-04-16 21:18:21] [INFO] model_generate_args.temperature: 1.0
[2025-04-16 21:18:21] [INFO] model_generate_args.top_p: 1.0
[2025-04-16 21:18:21] [INFO] model_generate_args.top_k: 50
[2025-04-16 21:18:21] [INFO] model_generate_args.do_sample: True
[2025-04-16 21:18:21] [INFO] model_generate_args.num_beams: 1
[2025-04-16 21:18:21] [INFO] model_generate_args.length_penalty: 1.0
[2025-04-16 21:18:21] [INFO] model_generate_args.use_cache: True
[2025-04-16 21:18:21] [INFO] model_generate_args.repetition_penalty: 1.0
[2025-04-16 21:18:21] [INFO] tokenizer_generate_args:
[2025-04-16 21:18:21] [INFO] tokenizer_generate_args.padding: True
[2025-04-16 21:18:21] [INFO] tokenizer_generate_args.add_special_tokens: True
[2025-04-16 21:18:21] [INFO] tokenizer_generate_args.return_tensors: pt
[2025-04-16 21:18:21] [INFO] tokenizer_init_args:
[2025-04-16 21:18:21] [INFO] tokenizer_init_args.padding_side: left
[2025-04-16 21:18:21] [INFO] tokenizer_init_args.trust_remote_code: True
[2025-04-16 21:18:21] [INFO] tokenizer_init_args.cache_dir: (None,)
[2025-04-16 21:18:21] [INFO] model_init_args:
[2025-04-16 21:18:21] [INFO] model_init_args.trust_remote_code: True
[2025-04-16 21:18:21] [INFO] model_init_args.torch_dtype: auto
[2025-04-16 21:18:21] [INFO] model_init_args.cache_dir: None
[2025-04-16 21:18:21] [INFO] apply_chat_template: False
[2025-04-16 21:18:21] [INFO] device_config:
[2025-04-16 21:18:21] [INFO] device_config.device: cuda:0
[2025-04-16 21:18:21] [INFO] device_id: 0
[2025-04-16 21:23:38] [INFO] 日志系统已初始化
[2025-04-16 21:23:38] [INFO] 日志文件路径: d:\Code\Python\LLM-inference\inference_module\utils\log\test.log
[2025-04-16 21:23:38] [INFO] Initializing ModelInference with the following configuration:
[2025-04-16 21:23:38] [INFO] model_name: qwen2.5-0.5B
[2025-04-16 21:23:38] [INFO] model_path: D:\Files\Models\
[2025-04-16 21:23:38] [INFO] chat_type: classical
[2025-04-16 21:23:38] [INFO] model_type: auto
[2025-04-16 21:23:38] [INFO] model_generate_args:
[2025-04-16 21:23:38] [INFO] model_generate_args.max_new_tokens: 1024
[2025-04-16 21:23:38] [INFO] model_generate_args.temperature: 1.0
[2025-04-16 21:23:38] [INFO] model_generate_args.top_p: 1.0
[2025-04-16 21:23:38] [INFO] model_generate_args.top_k: 50
[2025-04-16 21:23:38] [INFO] model_generate_args.do_sample: True
[2025-04-16 21:23:38] [INFO] model_generate_args.num_beams: 1
[2025-04-16 21:23:38] [INFO] model_generate_args.length_penalty: 1.0
[2025-04-16 21:23:38] [INFO] model_generate_args.use_cache: True
[2025-04-16 21:23:38] [INFO] model_generate_args.repetition_penalty: 1.0
[2025-04-16 21:23:38] [INFO] tokenizer_generate_args:
[2025-04-16 21:23:38] [INFO] tokenizer_generate_args.padding: True
[2025-04-16 21:23:38] [INFO] tokenizer_generate_args.add_special_tokens: True
[2025-04-16 21:23:38] [INFO] tokenizer_generate_args.return_tensors: pt
[2025-04-16 21:23:38] [INFO] tokenizer_init_args:
[2025-04-16 21:23:38] [INFO] tokenizer_init_args.padding_side: left
[2025-04-16 21:23:38] [INFO] tokenizer_init_args.trust_remote_code: True
[2025-04-16 21:23:38] [INFO] tokenizer_init_args.cache_dir: (None,)
[2025-04-16 21:23:38] [INFO] model_init_args:
[2025-04-16 21:23:38] [INFO] model_init_args.trust_remote_code: True
[2025-04-16 21:23:38] [INFO] model_init_args.torch_dtype: auto
[2025-04-16 21:23:38] [INFO] model_init_args.cache_dir: None
[2025-04-16 21:23:38] [INFO] apply_chat_template: False
[2025-04-16 21:23:38] [INFO] device_config:
[2025-04-16 21:23:38] [INFO] device_config.device: cuda:0
[2025-04-16 21:23:38] [INFO] device_id: 0
[2025-04-16 21:23:41] [WARNING] Failed to set special tokens for qwen2.5-0.5B
[2025-04-16 21:23:41] [INFO] ModelInference instance successfully initialized.
[2025-04-16 21:23:41] [INFO] Starting single inference...
[2025-04-16 21:24:01] [INFO] Single inference completed.
[2025-04-16 21:24:45] [INFO] 日志系统已初始化
[2025-04-16 21:24:45] [INFO] 日志文件路径: d:\Code\Python\LLM-inference\inference_module\utils\log\test.log
[2025-04-16 21:24:45] [INFO] Initializing ModelInference with the following configuration:
[2025-04-16 21:24:45] [INFO] model_name: qwen2.5-0.5B
[2025-04-16 21:24:45] [INFO] model_path: D:\Files\Models\
[2025-04-16 21:24:45] [INFO] chat_type: classical
[2025-04-16 21:24:45] [INFO] model_type: auto
[2025-04-16 21:24:45] [INFO] model_generate_args:
[2025-04-16 21:24:45] [INFO] model_generate_args.max_new_tokens: 1024
[2025-04-16 21:24:45] [INFO] model_generate_args.temperature: 1.0
[2025-04-16 21:24:45] [INFO] model_generate_args.top_p: 1.0
[2025-04-16 21:24:45] [INFO] model_generate_args.top_k: 50
[2025-04-16 21:24:45] [INFO] model_generate_args.do_sample: True
[2025-04-16 21:24:45] [INFO] model_generate_args.num_beams: 1
[2025-04-16 21:24:45] [INFO] model_generate_args.length_penalty: 1.0
[2025-04-16 21:24:45] [INFO] model_generate_args.use_cache: True
[2025-04-16 21:24:45] [INFO] model_generate_args.repetition_penalty: 1.0
[2025-04-16 21:24:45] [INFO] tokenizer_generate_args:
[2025-04-16 21:24:45] [INFO] tokenizer_generate_args.padding: True
[2025-04-16 21:24:45] [INFO] tokenizer_generate_args.add_special_tokens: True
[2025-04-16 21:24:45] [INFO] tokenizer_generate_args.return_tensors: pt
[2025-04-16 21:24:45] [INFO] tokenizer_init_args:
[2025-04-16 21:24:45] [INFO] tokenizer_init_args.padding_side: left
[2025-04-16 21:24:45] [INFO] tokenizer_init_args.trust_remote_code: True
[2025-04-16 21:24:45] [INFO] tokenizer_init_args.cache_dir: (None,)
[2025-04-16 21:24:45] [INFO] model_init_args:
[2025-04-16 21:24:45] [INFO] model_init_args.trust_remote_code: True
[2025-04-16 21:24:45] [INFO] model_init_args.torch_dtype: auto
[2025-04-16 21:24:45] [INFO] model_init_args.cache_dir: None
[2025-04-16 21:24:45] [INFO] apply_chat_template: False
[2025-04-16 21:24:45] [INFO] device_config:
[2025-04-16 21:24:45] [INFO] device_config.device: cuda:0
[2025-04-16 21:24:45] [INFO] device_id: 0
[2025-04-16 21:24:47] [WARNING] Failed to set special tokens for qwen2.5-0.5B
[2025-04-16 21:24:47] [INFO] ModelInference instance successfully initialized.
[2025-04-16 21:24:47] [INFO] Starting single inference...
[2025-04-16 21:25:08] [INFO] Single inference completed.
[2025-04-16 21:27:44] [INFO] 日志系统已初始化
[2025-04-16 21:27:44] [INFO] 日志文件路径: d:\Code\Python\LLM-inference\inference_module\utils\log\test.log
[2025-04-16 21:27:44] [INFO] Initializing ModelInference with the following configuration:
[2025-04-16 21:27:44] [INFO] model_name: qwen2.5-0.5B
[2025-04-16 21:27:44] [INFO] model_path: D:\Files\Models\
[2025-04-16 21:27:44] [INFO] chat_type: classical
[2025-04-16 21:27:44] [INFO] model_type: auto
[2025-04-16 21:27:44] [INFO] model_generate_args:
[2025-04-16 21:27:44] [INFO] model_generate_args.max_new_tokens: 1024
[2025-04-16 21:27:44] [INFO] model_generate_args.temperature: 1.0
[2025-04-16 21:27:44] [INFO] model_generate_args.top_p: 1.0
[2025-04-16 21:27:44] [INFO] model_generate_args.top_k: 50
[2025-04-16 21:27:44] [INFO] model_generate_args.do_sample: True
[2025-04-16 21:27:44] [INFO] model_generate_args.num_beams: 1
[2025-04-16 21:27:44] [INFO] model_generate_args.length_penalty: 1.0
[2025-04-16 21:27:44] [INFO] model_generate_args.use_cache: True
[2025-04-16 21:27:44] [INFO] model_generate_args.repetition_penalty: 1.0
[2025-04-16 21:27:44] [INFO] tokenizer_generate_args:
[2025-04-16 21:27:44] [INFO] tokenizer_generate_args.padding: True
[2025-04-16 21:27:44] [INFO] tokenizer_generate_args.add_special_tokens: True
[2025-04-16 21:27:44] [INFO] tokenizer_generate_args.return_tensors: pt
[2025-04-16 21:27:44] [INFO] tokenizer_init_args:
[2025-04-16 21:27:44] [INFO] tokenizer_init_args.padding_side: left
[2025-04-16 21:27:44] [INFO] tokenizer_init_args.trust_remote_code: True
[2025-04-16 21:27:44] [INFO] tokenizer_init_args.cache_dir: (None,)
[2025-04-16 21:27:44] [INFO] model_init_args:
[2025-04-16 21:27:44] [INFO] model_init_args.trust_remote_code: True
[2025-04-16 21:27:44] [INFO] model_init_args.torch_dtype: auto
[2025-04-16 21:27:44] [INFO] model_init_args.cache_dir: None
[2025-04-16 21:27:44] [INFO] apply_chat_template: False
[2025-04-16 21:27:44] [INFO] device_config:
[2025-04-16 21:27:44] [INFO] device_config.device: cuda:0
[2025-04-16 21:27:44] [INFO] device_id: 0
[2025-04-16 21:27:46] [INFO] Added missing special tokens for qwen2.5-0.5B
[2025-04-16 21:27:46] [INFO] ModelInference instance successfully initialized.
[2025-04-16 21:27:46] [INFO] Starting single inference...
[2025-04-16 21:28:06] [INFO] Single inference completed.
[2025-04-16 21:29:53] [INFO] 日志系统已初始化
[2025-04-16 21:29:53] [INFO] 日志文件路径: d:\Code\Python\LLM-inference\inference_module\utils\log\test.log
[2025-04-16 21:29:53] [INFO] Initializing ModelInference with the following configuration:
[2025-04-16 21:29:53] [INFO] model_name: qwen2.5-0.5B
[2025-04-16 21:29:53] [INFO] model_path: D:\Files\Models\
[2025-04-16 21:29:53] [INFO] chat_type: classical
[2025-04-16 21:29:53] [INFO] model_type: auto
[2025-04-16 21:29:53] [INFO] model_generate_args:
[2025-04-16 21:29:53] [INFO] model_generate_args.max_new_tokens: 1024
[2025-04-16 21:29:53] [INFO] model_generate_args.temperature: 1.0
[2025-04-16 21:29:53] [INFO] model_generate_args.top_p: 1.0
[2025-04-16 21:29:53] [INFO] model_generate_args.top_k: 50
[2025-04-16 21:29:53] [INFO] model_generate_args.do_sample: True
[2025-04-16 21:29:53] [INFO] model_generate_args.num_beams: 1
[2025-04-16 21:29:53] [INFO] model_generate_args.length_penalty: 1.0
[2025-04-16 21:29:53] [INFO] model_generate_args.use_cache: True
[2025-04-16 21:29:53] [INFO] model_generate_args.repetition_penalty: 1.0
[2025-04-16 21:29:53] [INFO] tokenizer_generate_args:
[2025-04-16 21:29:53] [INFO] tokenizer_generate_args.padding: True
[2025-04-16 21:29:53] [INFO] tokenizer_generate_args.add_special_tokens: True
[2025-04-16 21:29:53] [INFO] tokenizer_generate_args.return_tensors: pt
[2025-04-16 21:29:53] [INFO] tokenizer_init_args:
[2025-04-16 21:29:53] [INFO] tokenizer_init_args.padding_side: left
[2025-04-16 21:29:53] [INFO] tokenizer_init_args.trust_remote_code: True
[2025-04-16 21:29:53] [INFO] tokenizer_init_args.cache_dir: (None,)
[2025-04-16 21:29:53] [INFO] model_init_args:
[2025-04-16 21:29:53] [INFO] model_init_args.trust_remote_code: True
[2025-04-16 21:29:53] [INFO] model_init_args.torch_dtype: auto
[2025-04-16 21:29:53] [INFO] model_init_args.cache_dir: None
[2025-04-16 21:29:53] [INFO] apply_chat_template: False
[2025-04-16 21:29:53] [INFO] device_config:
[2025-04-16 21:29:53] [INFO] device_config.device: cuda:0
[2025-04-16 21:29:53] [INFO] device_id: 0
[2025-04-16 21:29:55] [INFO] Added missing special tokens for qwen2.5-0.5B
[2025-04-16 21:29:55] [INFO] ModelInference instance successfully initialized.
[2025-04-16 21:31:27] [INFO] 日志系统已初始化
[2025-04-16 21:31:27] [INFO] 日志文件路径: d:\Code\Python\LLM-inference\inference_module\utils\log\test.log
[2025-04-16 21:31:27] [INFO] Initializing ModelInference with the following configuration:
[2025-04-16 21:31:27] [INFO] model_name: qwen2.5-0.5B
[2025-04-16 21:31:27] [INFO] model_path: D:\Files\Models\
[2025-04-16 21:31:27] [INFO] chat_type: classical
[2025-04-16 21:31:27] [INFO] model_type: auto
[2025-04-16 21:31:27] [INFO] model_generate_args:
[2025-04-16 21:31:27] [INFO] model_generate_args.max_new_tokens: 1024
[2025-04-16 21:31:27] [INFO] model_generate_args.temperature: 1.0
[2025-04-16 21:31:27] [INFO] model_generate_args.top_p: 1.0
[2025-04-16 21:31:27] [INFO] model_generate_args.top_k: 50
[2025-04-16 21:31:27] [INFO] model_generate_args.do_sample: True
[2025-04-16 21:31:27] [INFO] model_generate_args.num_beams: 1
[2025-04-16 21:31:27] [INFO] model_generate_args.length_penalty: 1.0
[2025-04-16 21:31:27] [INFO] model_generate_args.use_cache: True
[2025-04-16 21:31:27] [INFO] model_generate_args.repetition_penalty: 1.0
[2025-04-16 21:31:27] [INFO] tokenizer_generate_args:
[2025-04-16 21:31:27] [INFO] tokenizer_generate_args.padding: True
[2025-04-16 21:31:27] [INFO] tokenizer_generate_args.add_special_tokens: True
[2025-04-16 21:31:27] [INFO] tokenizer_generate_args.return_tensors: pt
[2025-04-16 21:31:27] [INFO] tokenizer_init_args:
[2025-04-16 21:31:27] [INFO] tokenizer_init_args.padding_side: left
[2025-04-16 21:31:27] [INFO] tokenizer_init_args.trust_remote_code: True
[2025-04-16 21:31:27] [INFO] tokenizer_init_args.cache_dir: (None,)
[2025-04-16 21:31:27] [INFO] model_init_args:
[2025-04-16 21:31:27] [INFO] model_init_args.trust_remote_code: True
[2025-04-16 21:31:27] [INFO] model_init_args.torch_dtype: auto
[2025-04-16 21:31:27] [INFO] model_init_args.cache_dir: None
[2025-04-16 21:31:27] [INFO] apply_chat_template: False
[2025-04-16 21:31:27] [INFO] device_config:
[2025-04-16 21:31:27] [INFO] device_config.device: cuda:0
[2025-04-16 21:31:27] [INFO] device_id: 0
[2025-04-16 21:31:30] [INFO] Added missing special tokens for qwen2.5-0.5B
[2025-04-16 21:31:30] [INFO] ModelInference instance successfully initialized.
[2025-04-16 21:40:21] [INFO] 日志系统已初始化
[2025-04-16 21:40:21] [INFO] 日志文件路径: d:\Code\Python\LLM-inference\inference_module\utils\log\test.log
[2025-04-16 21:40:21] [INFO] Initializing ModelInference with the following configuration:
[2025-04-16 21:40:21] [INFO] model_name: qwen2.5-0.5B
[2025-04-16 21:40:21] [INFO] model_path: D:\Files\Models\
[2025-04-16 21:40:21] [INFO] chat_type: classical
[2025-04-16 21:40:21] [INFO] model_type: auto
[2025-04-16 21:40:21] [INFO] model_generate_args:
[2025-04-16 21:40:21] [INFO] model_generate_args.max_new_tokens: 1024
[2025-04-16 21:40:21] [INFO] model_generate_args.temperature: 1.0
[2025-04-16 21:40:21] [INFO] model_generate_args.top_p: 1.0
[2025-04-16 21:40:21] [INFO] model_generate_args.top_k: 50
[2025-04-16 21:40:21] [INFO] model_generate_args.do_sample: True
[2025-04-16 21:40:21] [INFO] model_generate_args.num_beams: 1
[2025-04-16 21:40:21] [INFO] model_generate_args.length_penalty: 1.0
[2025-04-16 21:40:21] [INFO] model_generate_args.use_cache: True
[2025-04-16 21:40:21] [INFO] model_generate_args.repetition_penalty: 1.0
[2025-04-16 21:40:21] [INFO] tokenizer_generate_args:
[2025-04-16 21:40:21] [INFO] tokenizer_generate_args.padding: True
[2025-04-16 21:40:21] [INFO] tokenizer_generate_args.add_special_tokens: True
[2025-04-16 21:40:21] [INFO] tokenizer_generate_args.return_tensors: pt
[2025-04-16 21:40:21] [INFO] tokenizer_init_args:
[2025-04-16 21:40:21] [INFO] tokenizer_init_args.padding_side: left
[2025-04-16 21:40:21] [INFO] tokenizer_init_args.trust_remote_code: True
[2025-04-16 21:40:21] [INFO] tokenizer_init_args.cache_dir: (None,)
[2025-04-16 21:40:21] [INFO] model_init_args:
[2025-04-16 21:40:21] [INFO] model_init_args.trust_remote_code: True
[2025-04-16 21:40:21] [INFO] model_init_args.torch_dtype: auto
[2025-04-16 21:40:21] [INFO] model_init_args.cache_dir: None
[2025-04-16 21:40:21] [INFO] apply_chat_template: False
[2025-04-16 21:40:21] [INFO] device_config:
[2025-04-16 21:40:21] [INFO] device_config.device: cuda:0
[2025-04-16 21:40:21] [INFO] device_id: 0
[2025-04-16 21:40:23] [INFO] Added missing special tokens for qwen2.5-0.5B
[2025-04-16 21:40:23] [INFO] ModelInference instance successfully initialized.
